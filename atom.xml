<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Causes Engineering]]></title>
  <link href="http://causes.github.io/atom.xml" rel="self"/>
  <link href="http://causes.github.io/"/>
  <updated>2013-05-30T17:34:36-07:00</updated>
  <id>http://causes.github.io/</id>
  <author>
    <name><![CDATA[Causes Engineers]]></name>
    <email><![CDATA[eng@causes.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Overcommit: The opinionated Git hook manager]]></title>
    <link href="http://causes.github.io/blog/2013/05/30/overcommit-the-opinionated-git-hook-manager/"/>
    <updated>2013-05-30T14:02:00-07:00</updated>
    <id>http://causes.github.io/blog/2013/05/30/overcommit-the-opinionated-git-hook-manager</id>
    <content type="html"><![CDATA[<p>At <a href="http://www.causes.com">Causes</a>, we care deeply about code quality.  We
promote thorough, offline code review through
<a href="https://code.google.com/p/gerrit/">Gerrit</a> and take pride in each commit we
make. Due to the sheer volume of code review and the number of engineers on our
team, it&rsquo;s important that by the time other engineers review our code we have an
established baseline of quality.</p>

<p>There are a few important ingredients to making a good commit:</p>

<ul>
<li>Correctness: The code does what you expect it to do</li>
<li>Commit message: Tim Pope provides an excellent summary of <a href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html">what makes a good
commit message</a></li>
<li>Style: The code matches our team&rsquo;s coding styles</li>
<li>Test coverage: relevant tests have been run, and any new features have spec
coverage</li>
</ul>


<p>Enter <a href="https://github.com/causes/overcommit">overcommit</a>. This evolved from a
single file linter into a full-fledged, extensible hook architecture.  Available
as a Ruby gem:</p>

<pre><code>gem install overcommit
</code></pre>

<p>What does it do? In short, it automates away all the tedium before a commit
reaches code review. It ships with a set of opinionated lints that ensure a
level of consistency and quality in our commits.</p>

<h2>In Action</h2>

<p>Here&rsquo;s an example of overcommit saving me from committing janky code:</p>

<pre><code>❯❯❯ echo "eval('alert(\"hello world\")');" > eval.js
❯❯❯ git add eval.js
❯❯❯ git commit
Running pre_commit checks
  Checking causes_email...........<span class="success">OK</span>
  Checking test_history...........<span class="warning">No relevant tests for this change...write some?</span>
  Checking restricted_paths.......<span class="success">OK</span>
  Checking js_console_log.........<span class="success">OK</span>
  Checking js_syntax..............<span class="error">FAILED</span>
    ERROR in eval.js at 1:1

        eval is evil.

        eval('alert("hello world")');

  Checking author_name............<span class="success">OK</span>
  Checking whitespace.............<span class="success">OK</span>

<span class="error">!!! One or more pre_commit checks failed</span>
</code></pre>




<!-- more -->


<h2>Installation</h2>

<p>After installing the gem, a new binary, <code>overcommit</code>, will be available. You can
use this binary to install git hooks into an existing repository like so:</p>

<pre><code>overcommit my-project
</code></pre>

<p>Where <code>my-project</code> is the directory of a git repository. In addition to
installing hooks into <code>my-project/.git/hooks</code>, this will also write an
<code>overcommit.yml</code> file containing repository-specific configuration. You can use
this, for example, to always skip a certain type of lint. See more options by
running <code>overcommit --help</code>.</p>

<h2>Built-in functionality</h2>

<h4><code>pre-commit</code> hooks</h4>

<ul>
<li><p><code>haml_syntax</code> verifies that any <a href="http://haml.info/">Haml</a> file to be committed
is syntactially valid.</p></li>
<li><p><code>js_syntax</code> uses <a href="http://www.jshint.com/">jshint</a> to ensure best JavaScript
practices are followed.</p></li>
<li><p><code>ruby_syntax</code> makes sure that any Ruby files to be committed are syntactically
valid by  running <code>ruby -c #{staged_file}</code>.</p></li>
<li><p><code>scss_lint</code> integrates with our <a href="github.com/causes/scss-lint">scss-lint</a> gem
to ensure our stylesheets are the best they can be. This includes alphabetizing
properties, removing unnecessary units, and so much more. <a href="https://github.com/causes/scss-lint#what-gets-linted">Read
more</a> about what gets
linted.</p></li>
<li><p><code>whitespace</code> verifies that no hard tabs are used and that no trailing
whitespace is included. The devil is in the details.</p></li>
</ul>


<p>There are more hooks included, including Causes-specific ones (such as making
sure your @causes.com email address is used), but these are excluded by
default. See the rest of the lints
<a href="https://github.com/causes/overcommit/tree/master/lib/overcommit/plugins/pre_commit">here</a>.</p>

<h4><code>commit-msg</code> hooks</h4>

<ul>
<li><p><code>russian_novel</code> is just for fun, to reward developers for writing exemplary
(long) commit messages.</p></li>
<li><p><code>text_width</code> ensures that the body of the commit is hard-wrapped to 72
characters, and that the subject is &lt;= 60 characters.</p></li>
<li><p><code>trailing_period</code> warns the author if their commit message subject ends with a period.</p></li>
</ul>


<h2>Extensibility</h2>

<p>In addition to the gem-provided, global hooks, <code>overcommit</code> also allows for
repository-specific hooks to be added. An example from the documentation is our
<a href="http://www.opscode.com/chef/">Chef</a> repository-specific hook to run
<a href="http://acrmp.github.io/foodcritic/">Foodcritic</a> against any cookbooks being
committed.</p>

<p>This file lives in kitchen.git/.githooks/pre_commit/food_critic.rb</p>

<figure class='code'><figcaption><span>food_critic.rb</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">module</span> <span class="nn">Overcommit::GitHook</span>
</span><span class='line'>  <span class="k">class</span> <span class="nc">FoodCritic</span> <span class="o">&lt;</span> <span class="no">HookSpecificCheck</span>
</span><span class='line'>    <span class="kp">include</span> <span class="no">HookRegistroy</span>
</span><span class='line'>    <span class="no">COOKBOOKS</span> <span class="o">=</span> <span class="s1">&#39;cookbooks&#39;</span>
</span><span class='line'>    <span class="vc">@@options</span> <span class="o">=</span> <span class="p">{</span> <span class="ss">:tags</span> <span class="o">=&gt;</span> <span class="sx">%w[~readme ~fc001]</span> <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">run_check</span>
</span><span class='line'>      <span class="k">begin</span>
</span><span class='line'>        <span class="nb">require</span> <span class="s1">&#39;foodcritic&#39;</span>
</span><span class='line'>      <span class="k">rescue</span> <span class="no">LoadError</span>
</span><span class='line'>        <span class="k">return</span> <span class="ss">:stop</span><span class="p">,</span> <span class="s1">&#39;run `bundle install` to install the foodcritic gem&#39;</span>
</span><span class='line'>      <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>      <span class="n">changed_cookbooks</span> <span class="o">=</span> <span class="n">modified_files</span><span class="o">.</span><span class="n">map</span> <span class="k">do</span> <span class="o">|</span><span class="n">file</span><span class="o">|</span>
</span><span class='line'>        <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span><span class="o">[</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="mi">1</span><span class="o">].</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">start_with?</span> <span class="no">COOKBOOKS</span>
</span><span class='line'>      <span class="k">end</span><span class="o">.</span><span class="n">compact</span><span class="o">.</span><span class="n">uniq</span>
</span><span class='line'>
</span><span class='line'>      <span class="n">linter</span> <span class="o">=</span> <span class="o">::</span><span class="ss">FoodCritic</span><span class="p">:</span><span class="ss">:Linter</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'>      <span class="n">review</span> <span class="o">=</span> <span class="n">linter</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">changed_cookbooks</span><span class="p">,</span> <span class="vc">@@options</span><span class="p">)</span>
</span><span class='line'>      <span class="k">return</span> <span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">warnings</span><span class="o">.</span><span class="n">any?</span> <span class="p">?</span> <span class="ss">:bad</span> <span class="p">:</span> <span class="ss">:good</span><span class="p">),</span> <span class="n">review</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Skipping checks</h2>

<p>There are, of course, times when you&rsquo;re going to need to break the rules. You
can skip any lint by passing the underscore-ized name into the <code>SKIP_CHECKS</code>
environment variable. This can either be a single lint, or a
comma/colon-separated list of lints to skip:</p>

<pre><code>SKIP_CHECKS=js_syntax:restricted_paths git commit
</code></pre>

<p>There&rsquo;s also the special value of <code>all</code>, which will skip all of the non-required
lints.</p>

<h2>The future</h2>

<p>Until recently, overcommit has only been useful inside Causes due to the
specific lints we run and the lack of easy installation. We hope others find it
useful, and have released it under the MIT license. Pull requests are welcome.
<a href="https://github.com/causes/overcommit">View the source on GitHub</a>.</p>

<p>Happy hacking.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Working with Asynchronously Loaded JavaScript Objects]]></title>
    <link href="http://causes.github.io/blog/2013/05/28/working-with-asynchronously-loaded-javascript-objects/"/>
    <updated>2013-05-28T14:00:00-07:00</updated>
    <id>http://causes.github.io/blog/2013/05/28/working-with-asynchronously-loaded-javascript-objects</id>
    <content type="html"><![CDATA[<p>Telling browsers to load large JavaScripts asynchronously can significantly
improve performance. This prevents the browser from blocking the rendering of
the page, allowing it to be viewed more quickly. However, if you are loading
dependent files asynchronously, such as a third-party service&rsquo;s API, making the
scripts work together is not automatic.</p>

<p>At <a href="http://www.causes.com">Causes</a> we use Facebook&rsquo;s large (nearly 60 <abbr
title="Kibibytes">KiB</abbr> gzipped) JavaScript API on our pages. Although
<a href="https://developers.facebook.com/docs/reference/javascript/#loading">they recommend loading it
asynchronously</a>,
we were already putting our JavaScript at the bottom of the page and weren&rsquo;t
convinced that async would give us much additional benefit. However, after some
non-scientific performance tests it appeared that switching to asynchronously
loading the Facebook API could reduce the time to <code>DOMContentLoaded</code> by nearly
a full second on our pages.</p>

<!-- more -->


<p>After the Facebook API has been loaded asynchronously, it executes
<code>window.fbAsyncInit</code>, which is where  <a href="https://developers.facebook.com/docs/javascript/gettingstarted/#loading">they
suggest</a>
placing code that depends on the <code>FB</code>.</p>

<p>The trouble is, we wanted our JavaScript that interacts with the Facebook API
to continue to work even though it is dispersed throughout our scripts, and it
may execute before the API was available.</p>

<p>The <a href="http://stackoverflow.com/questions/3548493/how-to-detect-when-facebooks-fb-init-is-complete">answers on
StackOverflow</a>
may work, but they feel inelegant. For instance, <a href="http://stackoverflow.com/a/3549043/18986">one
answer</a> addresses this problem by
setting a flag to true in the <code>fbAsyncInit</code> callback, and creates a
<code>fbEnsureInit</code> method that uses a recursive timeout to poll the flag.</p>

<p>We decided to take a different approach. Thankfully, the Facebook API sets up a
single object, <code>FB</code>, for developers to interact with.</p>

<p>So we developed a class, <code>MethodProxy</code>, to act as a proxy between the rest of
our code and the <code>FB</code> object. It accepts an object to forward calls onto, and
it masquerades as an Array by implementing a <code>push</code> method. The <code>push</code> method
accepts a single argument of a array whose first position is always a string of
the method name on the <code>FB</code> object to be executed, followed by as many
arguments to be given to that method.</p>

<figure class='code'><figcaption><span>MethodProxy.push()</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="c1">// payload : [&#39;methodName&#39;, arguments*]</span>
</span><span class='line'><span class="k">this</span><span class="p">.</span><span class="nx">push</span> <span class="o">=</span> <span class="k">this</span><span class="p">.</span><span class="nx">forward</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(</span><span class="nx">payload</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">methodName</span> <span class="o">=</span> <span class="nx">payload</span><span class="p">.</span><span class="nx">shift</span><span class="p">().</span><span class="nx">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">),</span>
</span><span class='line'>      <span class="nx">object</span>     <span class="o">=</span> <span class="k">this</span><span class="p">.</span><span class="nx">object</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">method</span><span class="p">;</span>
</span><span class='line'>  <span class="k">while</span> <span class="p">(</span><span class="nx">methodName</span><span class="p">.</span><span class="nx">length</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="c1">// dig into the object as many levels as needed (e.g. `FB.XFBML.parse`)</span>
</span><span class='line'>    <span class="k">if</span> <span class="p">(</span><span class="nx">method</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">object</span> <span class="o">=</span> <span class="nx">object</span><span class="p">[</span><span class="nx">method</span><span class="p">];</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="nx">method</span> <span class="o">=</span> <span class="nx">methodName</span><span class="p">.</span><span class="nx">shift</span><span class="p">();</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="k">return</span> <span class="nx">object</span><span class="p">[</span><span class="nx">method</span><span class="p">].</span><span class="nx">apply</span><span class="p">(</span><span class="nx">object</span><span class="p">,</span> <span class="nx">payload</span><span class="p">);</span>
</span><span class='line'><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure>


<p>This allows us to initialize a native Array when the page loads to collect
calls to Facebook&rsquo;s API. Then, in the <code>fbAsyncInit</code> callback, we Indiana Jones
style swap out the native array with an instantiation of our <code>MethodProxy</code>
object, which then consumes the queued events by executing them on the <code>FB</code>
object. The end result is a consistent API for our code to use regardless of
the state of Facebook&rsquo;s asynchronously loaded API.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">MyFB</span> <span class="o">=</span> <span class="nb">window</span><span class="p">.</span><span class="nx">MyFB</span> <span class="o">||</span> <span class="p">[];</span>
</span><span class='line'>
</span><span class='line'><span class="nx">MyFB</span><span class="p">.</span><span class="nx">push</span><span class="p">([</span><span class="s1">&#39;ui&#39;</span><span class="p">,</span> <span class="p">{</span> <span class="p">...</span> <span class="p">}]);</span>
</span><span class='line'>
</span><span class='line'><span class="nb">window</span><span class="p">.</span><span class="nx">fbAsyncInit</span> <span class="o">=</span> <span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>  <span class="nx">FB</span><span class="p">.</span><span class="nx">init</span><span class="p">({</span>
</span><span class='line'>    <span class="p">...</span>
</span><span class='line'>  <span class="p">});</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">MyFB</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">MethodProxy</span><span class="p">(</span><span class="nx">FB</span><span class="p">,</span> <span class="nx">MyFB</span><span class="p">);</span>
</span><span class='line'><span class="p">};</span>
</span><span class='line'>
</span><span class='line'><span class="nx">MyFB</span><span class="p">.</span><span class="nx">push</span><span class="p">([</span><span class="s1">&#39;FBML.parse&#39;</span><span class="p">,</span> <span class="p">...]);</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>MethodProxy</code> is general enough to be used for other similar cases so we&rsquo;ve
released it as <a href="https://github.com/causes/method-proxy-js">method-proxy-js, an open source
project</a> under the MIT license. Pull
requests welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[10 Easy Ways to Craft More Readable CSS]]></title>
    <link href="http://causes.github.io/blog/2013/03/16/10-easy-ways-to-craft-more-readable-css/"/>
    <updated>2013-03-16T15:45:00-07:00</updated>
    <id>http://causes.github.io/blog/2013/03/16/10-easy-ways-to-craft-more-readable-css</id>
    <content type="html"><![CDATA[<blockquote><p>Always code as if the [person] who ends up maintaining your code will be a
violent psychopath who knows where you live. Code for readability.
—<cite><a href="https://groups.google.com/d/msg/comp.lang.c++/rYCO5yn4lXw/oITtSkZOtoUJ">John Woods</a></cite></p></blockquote>

<p>Diving into a large, old piece of CSS typically is neither easy nor
pleasurable. I find that <strong>the biggest challenges in working with old CSS often
lie in understanding the purpose and interactions of the styles</strong>.</p>

<p>When styling new elements, we have the entire context of the implementation
immediately available, and it is easy to write styles that make sense to us at
that very moment. However, in a few weeks or to a fresh pair of eyes, what made
a lot of sense at first might end up being a lot more cryptic. Without a clear
understanding of the purpose and interactions of the styles, modifying
stylesheets can be dangerous, tedious, and cumbersome. Therefore, it is
important to communicate enough context so that future developers will be able
to grok the code easily and make informed decisions.</p>

<p>At <a href="http://www.causes.com/">Causes</a>, we have adopted the following practices
which we believe have improved the maintainability of our stylesheets, reduced
bugs, and increased developer velocity. When you have finished reading this, I
hope that you will have a few more tools to help move your codebase toward
greater maintainability.</p>

<p><a class="more-link" href="http://joelencioni.com/blog/2013/03/16/10-easy-ways-to-craft-more-readable-css/">Read on</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Even Faster: Loading Half a Billion Rows in MySQL Revisited]]></title>
    <link href="http://causes.github.io/blog/2012/08/21/even-faster-loading-half-a-billion-rows-in-mysql/"/>
    <updated>2012-08-21T00:00:00-07:00</updated>
    <id>http://causes.github.io/blog/2012/08/21/even-faster-loading-half-a-billion-rows-in-mysql</id>
    <content type="html"><![CDATA[<p>A few months ago, I wrote a post on loading 500 million rows into a single
innoDB table from flatfiles. This was in the effort to un-‘optimize’ a premature
optimization in our codebase: user action credits were being stored in monthly
sharded tables to keep the tables small and performant. As our use of the code
changed, we found more and more that we had to do a query for each month to see
if a user had taken an action. We implemented some performance optimizations
(mainly memcaching values from prior months as they are immutable), but it was
still overly complicated and prone to bugs. Since we had another table that was
900m rows, it seemed reasonable to collapse these shards into one 500m row
table.</p>

<p>Since writing the last post, I’ve learned that there’s a much quicker way to
combine those tables — as long as you already have the data in MySQL. MySQL
allows for selecting from one table into another via the <code>INSERT INTO ..
SELECT</code> statement:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">dest_table</span> <span class="p">(</span><span class="k">values</span><span class="p">)</span> <span class="k">SELECT</span> <span class="k">values</span> <span class="k">FROM</span> <span class="n">source_table</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>which might look something like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">credits_new</span> <span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="n">activity_id</span><span class="p">,</span> <span class="n">created_at</span><span class="p">)</span>
</span><span class='line'><span class="k">SELECT</span> <span class="n">user_id</span><span class="p">,</span> <span class="n">activity_id</span><span class="p">,</span> <span class="n">created_at</span> <span class="k">FROM</span> <span class="n">credits</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This shouldn’t be surprising; an ALTER TABLE on an innoDB table creates a new
table with the new schema and copies the rows from the old table over to the
new table.</p>

<!-- more -->


<p>With large tables though, you still have to do a little leg work to get this to
work properly. First, the database machine needs enough space to hold both the
old table(s) and new tables on disk, until you’re able to delete the old table.
As you insert into the table, inserts will get slower and slower. This also
makes sense; as the table grows the indexes grow, and working with them gets
slower. You’ll want to break the loading into many chunks so that each
transaction completes in a reasonable amount of time. In practice, I’ve found
that the optimum chunk size gets smaller as the size of the table grows.
Starting at 100k rows is not unreasonable, but don’t be surprised if near the
end you need to drop it closer to 10k or 5k to keep the transaction to under 30
seconds. If you don’t keep the transaction time reasonable, the whole operation
could outright fail eventually with something like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="n">ERROR</span> <span class="mi">1197</span> <span class="p">(</span><span class="n">HY000</span><span class="p">):</span> <span class="n">Multi</span><span class="o">-</span><span class="k">statement</span> <span class="n">transaction</span> <span class="n">required</span> <span class="k">more</span> <span class="k">than</span>
</span><span class='line'><span class="err">‘</span><span class="n">max_binlog_cache_size</span><span class="err">’</span> <span class="n">bytes</span> <span class="k">of</span> <span class="k">storage</span><span class="p">;</span> <span class="n">increase</span> <span class="n">this</span> <span class="n">mysqld</span> <span class="k">variable</span> <span class="k">and</span>
</span><span class='line'><span class="n">try</span> <span class="n">again</span><span class="p">.</span>
</span></code></pre></td></tr></table></div></figure>


<p>It will certainly slow down (I wish I had kept a graph of timestamp vs rows
inserted, but it’s certainly de-motivating). You might also want a delay as to
not overwhelm the slave and cause replication to lag too far behind. I used
external files to store the values of both variables that were read from disk at
each iteration, so that I could change the values without interrupting the
script. Using the same file trick, each iteration wrote out the table name and
id of the last row read (in case something broke and I wanted to resume).</p>

<p>This is the script that was used to perform the migration. I left it running in
a tmux session and also used `watch -n 10 LAST_WRITTEN’ to monitor the progress.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="c1">#!/usr/bin/env ruby</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">file_variable</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</span><span class='line'>  <span class="no">File</span><span class="o">.</span><span class="n">readlines</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">first</span><span class="o">.</span><span class="n">to_i</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'>
</span><span class='line'><span class="no">TARGET_TABLE</span> <span class="o">=</span> <span class="s1">&#39;action_credits&#39;</span>
</span><span class='line'><span class="no">SCHEMA</span> <span class="o">=</span> <span class="o">&lt;&lt;-</span><span class="no">SQL</span>
</span><span class='line'><span class="sh">CREATE TABLE IF NOT EXISTS `action_credits` (</span>
</span><span class='line'><span class="sh">.. omitted for brevity ..</span>
</span><span class='line'><span class="sh"> ) ENGINE=InnoDB  DEFAULT CHARSET=utf8;</span>
</span><span class='line'><span class="no">SQL</span>
</span><span class='line'><span class="n">insert_columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">has_utm_campaign</span> <span class="p">?</span> <span class="n">columns</span> <span class="p">:</span> <span class="n">columns</span> <span class="o">+</span> <span class="o">[</span><span class="s1">&#39;utm_campaign&#39;</span><span class="o">]</span><span class="p">)</span><span class="o">.</span><span class="n">join</span> <span class="s1">&#39;,&#39;</span>
</span><span class='line'><span class="n">select_columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">has_utm_campaign</span> <span class="p">?</span> <span class="n">columns</span> <span class="p">:</span> <span class="n">columns</span> <span class="o">+</span> <span class="o">[</span><span class="s1">&#39;NULL&#39;</span><span class="o">]</span><span class="p">)</span><span class="o">.</span><span class="n">join</span> <span class="s1">&#39;,&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_variable</span><span class="p">(</span><span class="s1">&#39;CHUNK_SIZE&#39;</span><span class="p">)</span>
</span><span class='line'><span class="p">(</span><span class="mi">0</span><span class="o">.</span><span class="n">.</span><span class="p">(</span><span class="n">rows</span> <span class="o">/</span> <span class="n">chunk_size</span><span class="p">))</span><span class="o">.</span><span class="n">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span>
</span><span class='line'>  <span class="n">lower</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">chunk_size</span>
</span><span class='line'>  <span class="n">upper</span> <span class="o">=</span> <span class="n">lower</span> <span class="o">+</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class='line'>  <span class="n">query</span> <span class="o">=</span> <span class="o">&lt;&lt;-</span><span class="no">SQL</span><span class="o">.</span><span class="n">squish</span>
</span><span class='line'><span class="sh">    INSERT INTO \#{TARGET_TABLE} (\#{insert_columns})</span>
</span><span class='line'><span class="sh">    SELECT \#{select_columns} FROM \#{table}</span>
</span><span class='line'><span class="sh">    WHERE id BETWEEN \#{lower} AND \#{upper}</span>
</span><span class='line'><span class="no">  SQL</span>
</span><span class='line'>  <span class="n">query_start</span> <span class="o">=</span> <span class="no">Time</span><span class="o">.</span><span class="n">now</span>
</span><span class='line'>  <span class="n">q</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="kp">true</span><span class="p">)</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Finished at </span><span class="se">\#</span><span class="s2">{Time.now} in </span><span class="se">\#</span><span class="s2">{Time.now - query_start} seconds&quot;</span>
</span><span class='line'>
</span><span class='line'>  <span class="no">File</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;LAST_WRITTEN&#39;</span><span class="p">,</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span> <span class="p">{</span><span class="o">|</span><span class="n">f</span><span class="o">|</span> <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="o">[</span><span class="n">table</span><span class="p">,</span> <span class="n">upper</span><span class="o">].</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">))}</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">delay</span> <span class="o">=</span> <span class="n">file_variable</span><span class="p">(</span><span class="s1">&#39;DELAY&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s2">&quot;Waiting </span><span class="se">\#</span><span class="s2">{delay} seconds&quot;</span> <span class="k">if</span> <span class="n">delay</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class='line'>  <span class="nb">sleep</span> <span class="n">delay</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'><span class="nb">puts</span> <span class="s2">&quot;Finished in </span><span class="se">\#</span><span class="s2">{((Time.now - start) / 3600).to_i} hours&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>There’s a slight concern that dropping the table while the filesystem deletes
the ibd files (where the innoDB data lives) will lock the table for a long
period of time (see <a href="http://bugs.mysql.com/bug.php?id=41158">http://bugs.mysql.com/bug.php?id=41158</a>), but it wasn’t a
problem when I tried it on a 116 GB table. If you prefer to be paranoid (like I
was), there’s a trick you can use to unlink the table data asynchronously:
create a hard link to the table’s .ibd file before you DROP the table; the DROP
will only unlink one of the two hard links to the file. Afterward you can `rm
the_hardlink’ and your filesystem will remove the .ibd data. Using this method
in practice took me 5 seconds to DROP and 13 seconds to rm the hardlink.</p>

<p>If all you need to do is a simple alter table (and not something complicated
like combining sharded tables), I’d recommend using the pt-online-schema-change
tool provided by Percona. We’ll look at the details of that tool in a future
post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Loading half a billion rows into MySQL]]></title>
    <link href="http://causes.github.io/blog/2012/06/05/loading-half-a-billion-rows-into-mysql/"/>
    <updated>2012-06-05T00:00:00-07:00</updated>
    <id>http://causes.github.io/blog/2012/06/05/loading-half-a-billion-rows-into-mysql</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p>We have a legacy system in our production environment that keeps track of when
a user takes an action on Causes.com (joins a Cause, recruits a friend, etc).
I say legacy, but I really mean a prematurely-optimized system that I&rsquo;d like
to make less smart. This 500m record database is split across monthly sharded
tables. Seems like a great solution to scaling (and it is)&mdash;except that
we don&rsquo;t need it. And based on our usage pattern (e.g. to count a user&rsquo;s total
number of actions, we need to do query N tables), this leads to pretty severe
performance degradation issues. Even with memcache layer sitting in front of
old month tables, new features keep discovering new N-query performance
problems.  Noticing that we have another database happily chugging along with
900 million records, I decided to migrate the existing system into a single
table setup. The goals were:</p>

<ul>
<li>Reduce complexity. Querying one table is simpler than N tables.</li>
<li>Push as much complexity as possible to the database. The wrappers around the
month-sharding logic in Rails are slow and buggy.</li>
<li>Increase performance. Also related to one table query being simpler than N.</li>
</ul>


<h2>Alternative Proposed Solutions</h2>

<p><em>MySQL Partitioning:</em>
This was the most similar to our existing set up, since MySQL internally
stores the data into different tables. We decided against it because it seemed
likely that it wouldn&rsquo;t be much faster than our current solution (although
MySQL can internally do some optimizations to make sure you only look at
tables that could possibly have data you want). And it&rsquo;s still the same
complexity we were looking to reduce (and would further be the only database
set up in our system using partitioning).</p>

<p><em>Redis:</em>
Not really proposed as an alternative because the full dataset won&rsquo;t fit
into memory, but something we&rsquo;re considering loading a subset of the data into
to answer queries that we make a lot that MySQL isn&rsquo;t particularly good at
(e.g.  &lsquo;which of my friends have taken an action&rsquo; is quick using Redis&rsquo;s built
in <code>SET UNION</code> function). The new MySQL table might be performant enough that it
doesn&rsquo;t make sense to build a fast Redis version, so we&rsquo;re avoiding this as
possible premature optimization, especially with a technology we&rsquo;re not as
familiar with.</p>

<!-- more -->


<h2>Dumping the old data</h2>

<p>MySQL provides the <code>mysqldump</code> utility to allow quick dumping to disk:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mysqldump -T /var/lib/mysql/database_data database_name
</span></code></pre></td></tr></table></div></figure>


<p>This will produce a TSV file for each table in the database, and this is the
format that <code>LOAD INFILE</code> will be able to quickly load later on.</p>

<h2>Installing Percona 5.5</h2>

<p>We&rsquo;ll be building the new system with the latest-and-greatest in Percona
databases on CentOS 6.2:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>rpm -Uhv http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm
</span><span class='line'>yum install Percona-Server-shared-compat Percona-Server-client-55 Percona-Server-server-55 -y
</span></code></pre></td></tr></table></div></figure>


<p>[ open bug with the compat package:
<a href="https://bugs.launchpad.net/percona-server/+bug/908620">https://bugs.launchpad.net/percona-server/+bug/908620</a> ]</p>

<h2>Specify a directory for the InnoDB data</h2>

<p>This isn&rsquo;t exactly a performance tip, but I had to do some digging to get MySQL
to store data on a different partition. The first step is to make use your
my.cnf contains a</p>

<pre><code>datadir = /path/to/data
</code></pre>

<p>directive. Make sure /path/to/data is owned by mysql:mysql</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>chown -R mysql.mysql /path/to/data
</span></code></pre></td></tr></table></div></figure>


<p>and run:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mysql_install_db --user<span class="o">=</span>mysql --datadir<span class="o">=</span>/path/to/data
</span></code></pre></td></tr></table></div></figure>


<p>This will set up the directory structures that InnoDB uses to store data. This
is also useful if you&rsquo;re aborting a failed data load and want to wipe the slate
clean (if you don&rsquo;t specify a directory, /var/lib/mysql is used by default).
Just <code>rm -rf *</code> the directory and run the <code>mysql_install_db</code> command.</p>

<p>[* <a href="http://dev.mysql.com/doc/refman/5.5/en/mysql-install-db.html">http://dev.mysql.com/doc/refman/5.5/en/mysql-install-db.html</a> ]</p>

<h2>SQL Commands to Speed up the LOAD DATA</h2>

<p>You can tell MySQL to not enforce foreign key and uniqueness constraints:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">FOREIGN_KEY_CHECKS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">UNIQUE_CHECKS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>and drop the transaction isolation guarantee to UNCOMMITTED:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="k">SESSION</span> <span class="n">tx_isolation</span><span class="o">=</span><span class="s1">&#39;READ-UNCOMMITTED&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>and turn off the binlog with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">sql_log_bin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>And when you&rsquo;re done, don&rsquo;t forget to turn it back on with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">UNIQUE_CHECKS</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">FOREIGN_KEY_CHECKS</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="k">SESSION</span> <span class="n">tx_isolation</span><span class="o">=</span><span class="s1">&#39;READ-REPEATABLE&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s worth noting that a lot of resources will tell you to to use the &ldquo;DISABLE
KEYS&rdquo; directive and have the indices all built once all the data has been loaded
into the table. Unfortunately, InnoDB does not support this. I tried it, and
while it took only a few hours to load 500m rows, the data was unusable without
any indices. You could drop the indices completely and add them later, but with
a table size this big I didn&rsquo;t think it would help much.</p>

<p>Another red herring was turning off autocommit and committing after each <code>LOAD
DATA</code> statement. This was effectively the same thing as autocommitting, and
manually commiting led to <code>LOAD DATA</code> slowdowns a quarter of the way in.
[<a href="http://dev.mysql.com/doc/refman/5.1/en/alter-table.html">http://dev.mysql.com/doc/refman/5.1/en/alter-table.html</a>]</p>

<pre><code>- [&lt;http://dev.mysql.com/doc/refman/5.1/en/alter-table.html&gt;, search for
  'DISABLE KEYS']
- [ &lt;http://www.mysqlperformanceblog.com/2007/11/01/innodb-performance-optimization-basics/&gt; ]
</code></pre>

<h2>Performance adjustments made to my.cnf</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">-</span><span class="n">flush_log</span><span class="o">=</span><span class="s1">&#39;http://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit&#39;</span>
</span><span class='line'><span class="c1">-- #{ link_to flush_log, flush_log }</span>
</span><span class='line'><span class="c1">-- this loosens the frequency with which the data is flushed to disk</span>
</span><span class='line'><span class="c1">-- it&#39;s possible to lose a second or two of data this way in the event of a</span>
</span><span class='line'><span class="c1">-- system crash, but this is in a very controlled circumstance</span>
</span><span class='line'><span class="n">innodb_flush_log_at_trx_commit</span><span class="o">=</span><span class="mi">2</span>
</span><span class='line'><span class="c1">-- rule of thumb is 75% - 80% of total system memory</span>
</span><span class='line'><span class="n">innodb_buffer_pool_size</span><span class="o">=</span><span class="mi">16</span><span class="n">GB</span>
</span><span class='line'><span class="c1">-- don&#39;t let the OS cache what InnoDB is caching anyway</span>
</span><span class='line'><span class="c1">-- http://www.mysqlperformanceblog.com/2007/11/01/innodb-performance-optimization-basics/</span>
</span><span class='line'><span class="n">innodb_flush_method</span><span class="o">=</span><span class="n">O_DIRECT</span>
</span><span class='line'><span class="c1">-- don&#39;t double write the data</span>
</span><span class='line'><span class="c1">-- http://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_doublewrite</span>
</span><span class='line'><span class="n">innodb_doublewrite</span> <span class="o">=</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Use LOAD DATA INFILE</h2>

<p>This is the most optimized path toward bulk loading structured data into MySQL.
<a href="http://dev.mysql.com/doc/refman/5.5/en/insert-speed.html">8.2.2.1. Speed of <code>INSERT</code> Statements,</a>
predicts a ~20x speedup over a bulk <code>INSERT</code> (i.e. an <code>INSERT</code> with thousands of
rows in a single statement). See also
<a href="http://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-bulk-data-loading.html">8.5.4. Bulk Data Loading for InnoDB Tables,</a>
for a few more tips.</p>

<p>Not only is it faster, but in my experience with this migration, the INSERT
method will slow down faster than it can load data and effectively never finish
(last estimate I made was 60 days, but it was still slowing down).</p>

<p>INFILE must be in the directory that InnoDB is storing that database
information. If MySQL is in /var/lib/mysql, then mydatabase would be in
/var/lib/mysql/mydatabase. If you don&rsquo;t have access to that directory on the
server, you can use <code>LOAD DATA LOCAL INFILE</code>. In my testing, putting the file in
the proper place and using <code>LOAD DATA INFILE</code> increased load performance by
about 20%.</p>

<p><a href="http://dev.mysql.com/doc/refman/5.5/en/load-data.html">http://dev.mysql.com/doc/refman/5.5/en/load-data.html</a></p>

<h2>Perform your data transformation directly in MySQL</h2>

<p>Our old actioncredit system was unique on (MONTH(created_at), id), but the new
system is going to generate new autoincrementing IDs for each records as it&rsquo;s
loaded in chronological order. The problem was that my 50 GB of TSV data doesn&rsquo;t
match up to the new schema. Some scripts I had that would use Ruby to transform
the old row into the new row was laughably slow. I did some digging and found
out that you can tell MySQL to (quickly) throw away the data you don&rsquo;t want in
the load statement itself, using parameter binding:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INFILE</span> <span class="s1">&#39;data.csv&#39;</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">mytable</span>
</span><span class='line'><span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">&#39;\t&#39;</span> <span class="n">ENCLOSED</span> <span class="k">BY</span> <span class="s1">&#39;\&quot;&#39;</span>
</span><span class='line'><span class="p">(</span><span class="o">@</span><span class="n">throwaway</span><span class="p">),</span> <span class="n">user_id</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">created_at</span>
</span></code></pre></td></tr></table></div></figure>


<p>This statement is telling MySQL which fields are represented in data.csv.
@throwaway is a binding parameter; and in this case we want to discard it so
we&rsquo;re not going to bind it. If we wanted to insert a prefix, we could execute:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INFILE</span> <span class="s1">&#39;data.csv&#39;</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">mytable</span>
</span><span class='line'><span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">&#39;\t&#39;</span> <span class="n">ENCLOSED</span> <span class="k">BY</span> <span class="s1">&#39;\&quot;&#39;</span>
</span><span class='line'><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">user_id</span><span class="p">,</span> <span class="o">@</span><span class="n">action</span><span class="p">,</span> <span class="n">created_at</span>
</span><span class='line'><span class="k">SET</span> <span class="n">action</span><span class="o">=</span><span class="n">CONCAT</span><span class="p">(</span><span class="s1">&#39;prefix_&#39;</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>and every loaded row&rsquo;s `action&#8217; column will begin with the string &lsquo;prefix&rsquo;.</p>

<h2>Checking progress without disrupting the import</h2>

<p>If you&rsquo;re loading large data files and want to check the progress, you
definitely don&rsquo;t want to use `SELECT COUNT(*) FROM table&#8217;. This query will
degrade as the size of the table grows and slowdown the LOAD process. Instead
you can query:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">&gt;</span> <span class="k">SELECT</span> <span class="n">table_rows</span> <span class="k">FROM</span> <span class="n">information_schema</span><span class="p">.</span><span class="n">tables</span> <span class="k">WHERE</span> <span class="k">table_name</span> <span class="o">=</span> <span class="s1">&#39;table&#39;</span><span class="p">;</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="o">|</span> <span class="n">table_rows</span> <span class="o">|</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="o">|</span>   <span class="mi">27273886</span> <span class="o">|</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="mi">1</span> <span class="k">row</span> <span class="k">in</span> <span class="k">set</span> <span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">23</span> <span class="n">sec</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you want to watch/log the progress over time, you can craft a quick shell
command to poll the number of rows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span><span class="k">while</span> :; <span class="k">do </span>mysql -hlocalhost databasename -e <span class="s2">&quot;SELECT table_rows FROM information_schema.tables WHERE table_name = &#39;table&#39; \G ; &quot;</span> | grep rows | cut -d<span class="s1">&#39;:&#39;</span> -f2 | xargs <span class="nb">echo</span> <span class="sb">`</span>date +<span class="s2">&quot;%F %R&quot;</span><span class="sb">`</span> , | tee load.log <span class="o">&amp;&amp;</span> sleep 30; <span class="k">done</span>
</span><span class='line'>2012-05-29 18:16 , 32267244
</span><span class='line'>2012-05-29 18:16 , 32328002
</span><span class='line'>2012-05-29 18:17 , 32404189
</span><span class='line'>2012-05-29 18:17 , 32473936
</span><span class='line'>2012-05-29 18:18 , 32543698
</span><span class='line'>2012-05-29 18:18 , 32616939
</span><span class='line'>2012-05-29 18:19 , 32693198
</span></code></pre></td></tr></table></div></figure>


<p>The <code>tee</code> will echo to STDOUT as well as to <code>file.log</code>, the <code>\G</code> formats the
columns in the result set as rows, and the sleep gives it a pause between
loading.</p>

<h2>LOAD DATA chunking script</h2>

<p>I quickly discovered that throwing a 50m row TSV file at LOAD DATA was a good
way to have performance degrade to the point of not finishing. I settled on
using `split&#8217; to chunk data into one million rows per file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">for </span>month_table in action*.txt; <span class="k">do</span>
</span><span class='line'><span class="nb">echo</span> <span class="s2">&quot;$(date) splitting $month_table...&quot;</span>
</span><span class='line'>split -l 1000000 <span class="nv">$month_table</span> curmonth_
</span><span class='line'><span class="k">for </span>segment in curmonth_*; <span class="k">do</span>
</span><span class='line'><span class="k">  </span><span class="nb">echo</span> <span class="s2">&quot;On segment $segment&quot;</span>
</span><span class='line'>  <span class="nb">time </span>mysql -hlocalhost action_credit_silo <span class="s">&lt;&lt;-SQL</span>
</span><span class='line'><span class="s">    SET FOREIGN_KEY_CHECKS = 0;</span>
</span><span class='line'><span class="s">    SET UNIQUE_CHECKS = 0;</span>
</span><span class='line'><span class="s">    SET SESSION tx_isolation=&#39;READ-UNCOMMITTED&#39;;</span>
</span><span class='line'><span class="s">    SET sql_log_bin = 0;</span>
</span><span class='line'><span class="s">    LOAD DATA INFILE &#39;$segment&#39; INTO TABLE actioncredits</span>
</span><span class='line'><span class="s">    FIELDS TERMINATED by &#39;\t&#39; ENCLOSED BY &#39;\&quot;&#39;</span>
</span><span class='line'><span class="s">    (@throwawayId, action, user_id, target_user_id, cause_id, item_type, item_id, activity_id, created_at, utm_campaign) ;</span>
</span><span class='line'><span class="s">  SQL</span>
</span><span class='line'>    rm <span class="nv">$segment</span>
</span><span class='line'>  <span class="k">done</span>
</span><span class='line'><span class="k">  </span>mv <span class="nv">$month_table</span> <span class="nv">$month_table</span>.done
</span><span class='line'><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Wrap-up</h2>

<p>Over the duration of this script, I saw chunk load time increase from 1m40s to
around an hour per million inserts. This is however better than not finishing at
all, which I wasn&rsquo;t able to achieve until making all changes suggested in this
post and using the aforementioned `load.sh&#8217; script. Other tips:</p>

<ul>
<li>use as few indices as you can</li>
<li>loading the data in sequential order not only makes the loading faster, but
the resulting table will be faster</li>
<li>if you can load any of the data from MySQL (instead of a flat file
intermediary), it will be much faster. You can use the `INSERT INTO ..
SELECT&#8217; statement to copy data between tables quickly.</li>
</ul>


<p>UPDATE: Since writing this article, I&rsquo;ve found even faster ways to load
this kind of data.  You can read more in the follow-up:</p>

<p><a href="http://derwiki.tumblr.com/post/29892583773/even-faster-loading-half-a-billion-rows-in-mysql">Even faster: loading half a billion rows in MySQL revisited&#8217;</a></p>

<p>Thanks for reading drafts of this to Greg and Lann, two of my
super-smart coworkers at Causes. Check out
<a href="http://www.causes.com/jobs">causes.com/jobs</a> if this sort of work interests you!</p>
]]></content>
  </entry>
  
</feed>
