
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>Loading half a billion rows into MySQL - Causes Engineering</title>
	<meta name="author" content="Causes Engineers">

	
	<meta name="description" content="Loading Half a Billion Rows Into MySQL Background We have a legacy system in our production environment that keeps track of when
a user takes an &hellip;">
	

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="/atom.xml" rel="alternate" title="Causes Engineering" type="application/atom+xml">
	
	<link rel="canonical" href="http://causes.github.io/blog/2012/06/05/loading-half-a-billion-rows-into-mysql/">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,400,700' rel='stylesheet' type='text/css'>
	<!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
			<header id="header" class="inner"><div class="profilepic">
	<script src="/javascripts/md5.js"></script>
	<script type="text/javascript">
		document.write("<img src='http://www.gravatar.com/avatar/" + MD5("") + "?s=160' alt='Profile Picture' style='width: 160px;' />");
	</script>
</div>
<h1><a href="/">Causes Engineering</a></h1>
<p class="subtitle"></p>
<nav id="main-nav"><ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>
</nav>
</header>				
			</div>
		</div>	
		<div class="mid-col">
			
				
			
			<div class="mid-col-container">
				<div id="content" class="inner"><article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<h1 class="title" itemprop="name">Loading Half a Billion Rows Into MySQL</h1>
	<div class="entry-content" itemprop="articleBody"><h2>Background</h2>

<p>We have a legacy system in our production environment that keeps track of when
a user takes an action on Causes.com (joins a Cause, recruits a friend, etc).
I say legacy, but I really mean a prematurely-optimized system that I&rsquo;d like
to make less smart. This 500m record database is split across monthly sharded
tables. Seems like a great solution to scaling (and it is)&mdash;except that
we don&rsquo;t need it. And based on our usage pattern (e.g. to count a user&rsquo;s total
number of actions, we need to do query N tables), this leads to pretty severe
performance degradation issues. Even with memcache layer sitting in front of
old month tables, new features keep discovering new N-query performance
problems.  Noticing that we have another database happily chugging along with
900 million records, I decided to migrate the existing system into a single
table setup. The goals were:</p>

<ul>
<li>Reduce complexity. Querying one table is simpler than N tables.</li>
<li>Push as much complexity as possible to the database. The wrappers around the
month-sharding logic in Rails are slow and buggy.</li>
<li>Increase performance. Also related to one table query being simpler than N.</li>
</ul>


<h1>Alternative Proposed Solutions</h1>

<p><em>MySQL Partitioning:</em>
This was the most similar to our existing set up, since MySQL internally
stores the data into different tables. We decided against it because it seemed
likely that it wouldn&rsquo;t be much faster than our current solution (although
MySQL can internally do some optimizations to make sure you only look at
tables that could possibly have data you want). And it&rsquo;s still the same
complexity we were looking to reduce (and would further be the only database
set up in our system using partitioning).</p>

<p><em>Redis:</em>
Not really proposed as an alternative because the full dataset won&rsquo;t fit
into memory, but something we&rsquo;re considering loading a subset of the data into
to answer queries that we make a lot that MySQL isn&rsquo;t particularly good at
(e.g.  &lsquo;which of my friends have taken an action&rsquo; is quick using Redis&rsquo;s built
in <code>SET UNION</code> function). The new MySQL table might be performant enough that it
doesn&rsquo;t make sense to build a fast Redis version, so we&rsquo;re avoiding this as
possible premature optimization, especially with a technology we&rsquo;re not as
familiar with.</p>

<h2>Dumping the old data</h2>

<p>MySQL provides the <code>mysqldump</code> utility to allow quick dumping to disk:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mysqldump -T /var/lib/mysql/database_data database_name
</span></code></pre></td></tr></table></div></figure>


<p>This will produce a TSV file for each table in the database, and this is the
format that <code>LOAD INFILE</code> will be able to quickly load later on.</p>

<h2>Installing Percona 5.5</h2>

<p>We&rsquo;ll be building the new system with the latest-and-greatest in Percona
databases on CentOS 6.2:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>rpm -Uhv http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm
</span><span class='line'>yum install Percona-Server-shared-compat Percona-Server-client-55 Percona-Server-server-55 -y
</span></code></pre></td></tr></table></div></figure>


<p>[ open bug with the compat package:
<a href="https://bugs.launchpad.net/percona-server/+bug/908620">https://bugs.launchpad.net/percona-server/+bug/908620</a> ]</p>

<h2>Specify a directory for the InnoDB data</h2>

<p>This isn&rsquo;t exactly a performance tip, but I had to do some digging to get MySQL
to store data on a different partition. The first step is to make use your
my.cnf contains a</p>

<pre><code>datadir = /path/to/data
</code></pre>

<p>directive. Make sure /path/to/data is owned by mysql:mysql</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>chown -R mysql.mysql /path/to/data
</span></code></pre></td></tr></table></div></figure>


<p>and run:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mysql_install_db --user<span class="o">=</span>mysql --datadir<span class="o">=</span>/path/to/data
</span></code></pre></td></tr></table></div></figure>


<p>This will set up the directory structures that InnoDB uses to store data. This
is also useful if you&rsquo;re aborting a failed data load and want to wipe the slate
clean (if you don&rsquo;t specify a directory, /var/lib/mysql is used by default).
Just <code>rm -rf *</code> the directory and run the <code>mysql_install_db</code> command.</p>

<p>[* <a href="http://dev.mysql.com/doc/refman/5.5/en/mysql-install-db.html">http://dev.mysql.com/doc/refman/5.5/en/mysql-install-db.html</a> ]</p>

<h2>SQL Commands to Speed up the LOAD DATA</h2>

<p>You can tell MySQL to not enforce foreign key and uniqueness constraints:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">FOREIGN_KEY_CHECKS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">UNIQUE_CHECKS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>and drop the transaction isolation guarantee to UNCOMMITTED:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="k">SESSION</span> <span class="n">tx_isolation</span><span class="o">=</span><span class="s1">&#39;READ-UNCOMMITTED&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>and turn off the binlog with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">sql_log_bin</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>And when you&rsquo;re done, don&rsquo;t forget to turn it back on with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SET</span> <span class="n">UNIQUE_CHECKS</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="n">FOREIGN_KEY_CHECKS</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'><span class="k">SET</span> <span class="k">SESSION</span> <span class="n">tx_isolation</span><span class="o">=</span><span class="s1">&#39;READ-REPEATABLE&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s worth noting that a lot of resources will tell you to to use the &ldquo;DISABLE
KEYS&rdquo; directive and have the indices all built once all the data has been loaded
into the table. Unfortunately, InnoDB does not support this. I tried it, and
while it took only a few hours to load 500m rows, the data was unusable without
any indices. You could drop the indices completely and add them later, but with
a table size this big I didn&rsquo;t think it would help much.</p>

<p>Another red herring was turning off autocommit and committing after each <code>LOAD
DATA</code> statement. This was effectively the same thing as autocommitting, and
manually commiting led to <code>LOAD DATA</code> slowdowns a quarter of the way in.
[<a href="http://dev.mysql.com/doc/refman/5.1/en/alter-table.html">http://dev.mysql.com/doc/refman/5.1/en/alter-table.html</a>]</p>

<pre><code>- [&lt;http://dev.mysql.com/doc/refman/5.1/en/alter-table.html&gt;, search for
  'DISABLE KEYS']
- [ &lt;http://www.mysqlperformanceblog.com/2007/11/01/innodb-performance-optimization-basics/&gt; ]
</code></pre>

<h2>Performance adjustments made to my.cnf</h2>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">-</span><span class="n">flush_log</span><span class="o">=</span><span class="s1">&#39;http://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit&#39;</span>
</span><span class='line'><span class="c1">-- #{ link_to flush_log, flush_log }</span>
</span><span class='line'><span class="c1">-- this loosens the frequency with which the data is flushed to disk</span>
</span><span class='line'><span class="c1">-- it&#39;s possible to lose a second or two of data this way in the event of a</span>
</span><span class='line'><span class="c1">-- system crash, but this is in a very controlled circumstance</span>
</span><span class='line'><span class="n">innodb_flush_log_at_trx_commit</span><span class="o">=</span><span class="mi">2</span>
</span><span class='line'><span class="c1">-- rule of thumb is 75% - 80% of total system memory</span>
</span><span class='line'><span class="n">innodb_buffer_pool_size</span><span class="o">=</span><span class="mi">16</span><span class="n">GB</span>
</span><span class='line'><span class="c1">-- don&#39;t let the OS cache what InnoDB is caching anyway</span>
</span><span class='line'><span class="c1">-- http://www.mysqlperformanceblog.com/2007/11/01/innodb-performance-optimization-basics/</span>
</span><span class='line'><span class="n">innodb_flush_method</span><span class="o">=</span><span class="n">O_DIRECT</span>
</span><span class='line'><span class="c1">-- don&#39;t double write the data</span>
</span><span class='line'><span class="c1">-- http://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_doublewrite</span>
</span><span class='line'><span class="n">innodb_doublewrite</span> <span class="o">=</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Use LOAD DATA INFILE</h2>

<p>This is the most optimized path toward bulk loading structured data into MySQL.
<a href="http://dev.mysql.com/doc/refman/5.5/en/insert-speed.html">8.2.2.1. Speed of <code>INSERT</code> Statements,</a>
predicts a ~20x speedup over a bulk <code>INSERT</code> (i.e. an <code>INSERT</code> with thousands of
rows in a single statement). See also
<a href="http://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-bulk-data-loading.html">8.5.4. Bulk Data Loading for InnoDB Tables,</a>
for a few more tips.</p>

<p>Not only is it faster, but in my experience with this migration, the INSERT
method will slow down faster than it can load data and effectively never finish
(last estimate I made was 60 days, but it was still slowing down).</p>

<p>INFILE must be in the directory that InnoDB is storing that database
information. If MySQL is in /var/lib/mysql, then mydatabase would be in
/var/lib/mysql/mydatabase. If you don&rsquo;t have access to that directory on the
server, you can use <code>LOAD DATA LOCAL INFILE</code>. In my testing, putting the file in
the proper place and using <code>LOAD DATA INFILE</code> increased load performance by
about 20%.</p>

<p><a href="http://dev.mysql.com/doc/refman/5.5/en/load-data.html">http://dev.mysql.com/doc/refman/5.5/en/load-data.html</a></p>

<h2>Perform your data transformation directly in MySQL</h2>

<p>Our old actioncredit system was unique on (MONTH(created_at), id), but the new
system is going to generate new autoincrementing IDs for each records as it&rsquo;s
loaded in chronological order. The problem was that my 50 GB of TSV data doesn&rsquo;t
match up to the new schema. Some scripts I had that would use Ruby to transform
the old row into the new row was laughably slow. I did some digging and found
out that you can tell MySQL to (quickly) throw away the data you don&rsquo;t want in
the load statement itself, using parameter binding:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INFILE</span> <span class="s1">&#39;data.csv&#39;</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">mytable</span>
</span><span class='line'><span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">&#39;\t&#39;</span> <span class="n">ENCLOSED</span> <span class="k">BY</span> <span class="s1">&#39;\&quot;&#39;</span>
</span><span class='line'><span class="p">(</span><span class="o">@</span><span class="n">throwaway</span><span class="p">),</span> <span class="n">user_id</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">created_at</span>
</span></code></pre></td></tr></table></div></figure>


<p>This statement is telling MySQL which fields are represented in data.csv.
@throwaway is a binding parameter; and in this case we want to discard it so
we&rsquo;re not going to bind it. If we wanted to insert a prefix, we could execute:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INFILE</span> <span class="s1">&#39;data.csv&#39;</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">mytable</span>
</span><span class='line'><span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">&#39;\t&#39;</span> <span class="n">ENCLOSED</span> <span class="k">BY</span> <span class="s1">&#39;\&quot;&#39;</span>
</span><span class='line'><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">user_id</span><span class="p">,</span> <span class="o">@</span><span class="n">action</span><span class="p">,</span> <span class="n">created_at</span>
</span><span class='line'><span class="k">SET</span> <span class="n">action</span><span class="o">=</span><span class="n">CONCAT</span><span class="p">(</span><span class="s1">&#39;prefix_&#39;</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>and every loaded row&rsquo;s `action&#8217; column will begin with the string &lsquo;prefix&rsquo;.</p>

<h2>Checking progress without disrupting the import</h2>

<p>If you&rsquo;re loading large data files and want to check the progress, you
definitely don&rsquo;t want to use `SELECT COUNT(*) FROM table&#8217;. This query will
degrade as the size of the table grows and slowdown the LOAD process. Instead
you can query:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="o">&gt;</span> <span class="k">SELECT</span> <span class="n">table_rows</span> <span class="k">FROM</span> <span class="n">information_schema</span><span class="p">.</span><span class="n">tables</span> <span class="k">WHERE</span> <span class="k">table_name</span> <span class="o">=</span> <span class="s1">&#39;table&#39;</span><span class="p">;</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="o">|</span> <span class="n">table_rows</span> <span class="o">|</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="o">|</span>   <span class="mi">27273886</span> <span class="o">|</span>
</span><span class='line'><span class="o">+</span><span class="c1">------------+</span>
</span><span class='line'><span class="mi">1</span> <span class="k">row</span> <span class="k">in</span> <span class="k">set</span> <span class="p">(</span><span class="mi">0</span><span class="p">.</span><span class="mi">23</span> <span class="n">sec</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you want to watch/log the progress over time, you can craft a quick shell
command to poll the number of rows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span><span class="k">while</span> :; <span class="k">do </span>mysql -hlocalhost databasename -e <span class="s2">&quot;SELECT table_rows FROM information_schema.tables WHERE table_name = &#39;table&#39; \G ; &quot;</span> | grep rows | cut -d<span class="s1">&#39;:&#39;</span> -f2 | xargs <span class="nb">echo</span> <span class="sb">`</span>date +<span class="s2">&quot;%F %R&quot;</span><span class="sb">`</span> , | tee load.log <span class="o">&amp;&amp;</span> sleep 30; <span class="k">done</span>
</span><span class='line'>2012-05-29 18:16 , 32267244
</span><span class='line'>2012-05-29 18:16 , 32328002
</span><span class='line'>2012-05-29 18:17 , 32404189
</span><span class='line'>2012-05-29 18:17 , 32473936
</span><span class='line'>2012-05-29 18:18 , 32543698
</span><span class='line'>2012-05-29 18:18 , 32616939
</span><span class='line'>2012-05-29 18:19 , 32693198
</span></code></pre></td></tr></table></div></figure>


<p>The <code>tee</code> will echo to STDOUT as well as to <code>file.log</code>, the <code>\G</code> formats the
columns in the result set as rows, and the sleep gives it a pause between
loading.</p>

<h2>LOAD DATA chunking script</h2>

<p>I quickly discovered that throwing a 50m row TSV file at LOAD DATA was a good
way to have performance degrade to the point of not finishing. I settled on
using `split&#8217; to chunk data into one million rows per file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">for </span>month_table in action*.txt; <span class="k">do</span>
</span><span class='line'><span class="nb">echo</span> <span class="s2">&quot;$(date) splitting $month_table...&quot;</span>
</span><span class='line'>split -l 1000000 <span class="nv">$month_table</span> curmonth_
</span><span class='line'><span class="k">for </span>segment in curmonth_*; <span class="k">do</span>
</span><span class='line'><span class="k">  </span><span class="nb">echo</span> <span class="s2">&quot;On segment $segment&quot;</span>
</span><span class='line'>  <span class="nb">time </span>mysql -hlocalhost action_credit_silo <span class="s">&lt;&lt;-SQL</span>
</span><span class='line'><span class="s">    SET FOREIGN_KEY_CHECKS = 0;</span>
</span><span class='line'><span class="s">    SET UNIQUE_CHECKS = 0;</span>
</span><span class='line'><span class="s">    SET SESSION tx_isolation=&#39;READ-UNCOMMITTED&#39;;</span>
</span><span class='line'><span class="s">    SET sql_log_bin = 0;</span>
</span><span class='line'><span class="s">    LOAD DATA INFILE &#39;$segment&#39; INTO TABLE actioncredits</span>
</span><span class='line'><span class="s">    FIELDS TERMINATED by &#39;\t&#39; ENCLOSED BY &#39;\&quot;&#39;</span>
</span><span class='line'><span class="s">    (@throwawayId, action, user_id, target_user_id, cause_id, item_type, item_id, activity_id, created_at, utm_campaign) ;</span>
</span><span class='line'><span class="s">  SQL</span>
</span><span class='line'>    rm <span class="nv">$segment</span>
</span><span class='line'>  <span class="k">done</span>
</span><span class='line'><span class="k">  </span>mv <span class="nv">$month_table</span> <span class="nv">$month_table</span>.done
</span><span class='line'><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Wrap-up</h2>

<p>Over the duration of this script, I saw chunk load time increase from 1m40s to
around an hour per million inserts. This is however better than not finishing at
all, which I wasn&rsquo;t able to achieve until making all changes suggested in this
post and using the aforementioned `load.sh&#8217; script. Other tips:</p>

<ul>
<li>use as few indices as you can</li>
<li>loading the data in sequential order not only makes the loading faster, but
the resulting table will be faster</li>
<li>if you can load any of the data from MySQL (instead of a flat file
intermediary), it will be much faster. You can use the `INSERT INTO ..
SELECT&#8217; statement to copy data between tables quickly.</li>
</ul>


<p>UPDATE: Since writing this article, I&rsquo;ve found even faster ways to load
this kind of data.  You can read more in the follow-up:</p>

<p><a href="http://derwiki.tumblr.com/post/29892583773/even-faster-loading-half-a-billion-rows-in-mysql">Even faster: loading half a billion rows in MySQL revisited&#8217;</a></p>

<p>Thanks for reading drafts of this to Greg and Lann, two of my
super-smart coworkers at Causes. Check out
<a href="http://www.causes.com/jobs">causes.com/jobs</a> if this sort of work interests you!</p>
</div>

</article>

	<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
	
	
	<a class="addthis_button_tweet"></a>
	
	
	<a class="addthis_counter addthis_pill_style"></a>
	</div>
  <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid="></script>
</div>


</div>
			</div>
			<footer id="footer" class="inner"><p>
  Copyright &copy; 2013 - Causes Engineers -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

Design credit: <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a></footer>
			<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->




	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-31836-33']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>




		</div>
	</div>
</body>
</html>
